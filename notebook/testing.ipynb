{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718dcad0-6a51-4342-a758-3c092bb4b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75619edf-0e60-481d-9f66-f7a409deabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TF-IDF vectorizer\n",
    "vectorizer_loaded = joblib.load(os.path.join(os.getcwd(), '..', 'model', 'tfidf_vectorizer.pkl'))\n",
    "\n",
    "# Load the TF-IDF matrix\n",
    "tfidf_matrix_loaded = joblib.load(os.path.join(os.getcwd(), '..', 'model', 'tfidf_matrix.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30be879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', 'abstracts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7370d6b8-7a2d-4060-9bfd-b1c358a8bf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.7577\n",
      "Abstract:   Deep learning has achieved a great success in many areas, from computer\n",
      "vision to natural language processing, to game playing, and much more. Yet,\n",
      "what deep learning is really doing is still an open question. There are a lot\n",
      "of works in this direction. For example, [5] tried to explain deep learning by\n",
      "group renormalization, and [6] tried to explain deep learning from the view of\n",
      "functional approximation. In order to address this very crucial question, here\n",
      "we see deep learning from perspective of mechanical learning and learning\n",
      "machine (see [1], [2]). From this particular angle, we can see deep learning\n",
      "much better and answer with confidence: What deep learning is really doing? why\n",
      "it works well, how it works, and how much data is necessary for learning. We\n",
      "also will discuss advantages and disadvantages of deep learning at the end of\n",
      "this work.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.7360\n",
      "Abstract:   The great success of deep learning shows that its technology contains\n",
      "profound truth, and understanding its internal mechanism not only has important\n",
      "implications for the development of its technology and effective application in\n",
      "various fields, but also provides meaningful insights into the understanding of\n",
      "human brain mechanism. At present, most of the theoretical research on deep\n",
      "learning is based on mathematics. This dissertation proposes that the neural\n",
      "network of deep learning is a physical system, examines deep learning from\n",
      "three different perspectives: microscopic, macroscopic, and physical world\n",
      "views, answers multiple theoretical puzzles in deep learning by using physics\n",
      "principles. For example, from the perspective of quantum mechanics and\n",
      "statistical physics, this dissertation presents the calculation methods for\n",
      "convolution calculation, pooling, normalization, and Restricted Boltzmann\n",
      "Machine, as well as the selection of cost functions, explains why deep learning\n",
      "must be deep, what characteristics are learned in deep learning, why\n",
      "Convolutional Neural Networks do not have to be trained layer by layer, and the\n",
      "limitations of deep learning, etc., and proposes the theoretical direction and\n",
      "basis for the further development of deep learning now and in the future. The\n",
      "brilliance of physics flashes in deep learning, we try to establish the deep\n",
      "learning technology based on the scientific theory of physics.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6627\n",
      "Abstract:   Deep Learning is one of the newest trends in Machine Learning and Artificial\n",
      "Intelligence research. It is also one of the most popular scientific research\n",
      "trends now-a-days. Deep learning methods have brought revolutionary advances in\n",
      "computer vision and machine learning. Every now and then, new and new deep\n",
      "learning techniques are being born, outperforming state-of-the-art machine\n",
      "learning and even existing deep learning techniques. In recent years, the world\n",
      "has seen many major breakthroughs in this field. Since deep learning is\n",
      "evolving at a huge speed, its kind of hard to keep track of the regular\n",
      "advances especially for new researchers. In this paper, we are going to briefly\n",
      "discuss about recent advances in Deep Learning for past few years.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6404\n",
      "Abstract:   Over the past few years, deep learning has risen to the foreground as a topic\n",
      "of massive interest, mainly as a result of successes obtained in solving\n",
      "large-scale image processing tasks. There are multiple challenging mathematical\n",
      "problems involved in applying deep learning: most deep learning methods require\n",
      "the solution of hard optimisation problems, and a good understanding of the\n",
      "tradeoff between computational effort, amount of data and model complexity is\n",
      "required to successfully design a deep learning approach for a given problem. A\n",
      "large amount of progress made in deep learning has been based on heuristic\n",
      "explorations, but there is a growing effort to mathematically understand the\n",
      "structure in existing deep learning methods and to systematically design new\n",
      "deep learning methods to preserve certain types of structure in deep learning.\n",
      "In this article, we review a number of these directions: some deep neural\n",
      "networks can be understood as discretisations of dynamical systems, neural\n",
      "networks can be designed to have desirable properties such as invertibility or\n",
      "group equivariance, and new algorithmic frameworks based on conformal\n",
      "Hamiltonian systems and Riemannian manifolds to solve the optimisation problems\n",
      "have been proposed. We conclude our review of each of these topics by\n",
      "discussing some open problems that we consider to be interesting directions for\n",
      "future research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6379\n",
      "Abstract:   Deep learning has made significant breakthroughs in various fields of\n",
      "artificial intelligence. Advantages of deep learning include the ability to\n",
      "capture highly complicated features, weak involvement of human engineering,\n",
      "etc. However, it is still virtually impossible to use deep learning to analyze\n",
      "programs since deep architectures cannot be trained effectively with pure back\n",
      "propagation. In this pioneering paper, we propose the \"coding criterion\" to\n",
      "build program vector representations, which are the premise of deep learning\n",
      "for program analysis. Our representation learning approach directly makes deep\n",
      "learning a reality in this new field. We evaluate the learned vector\n",
      "representations both qualitatively and quantitatively. We conclude, based on\n",
      "the experiments, the coding criterion is successful in building program\n",
      "representations. To evaluate whether deep learning is beneficial for program\n",
      "analysis, we feed the representations to deep neural networks, and achieve\n",
      "higher accuracy in the program classification task than \"shallow\" methods, such\n",
      "as logistic regression and the support vector machine. This result confirms the\n",
      "feasibility of deep learning to analyze programs. It also gives primary\n",
      "evidence of its success in this new field. We believe deep learning will become\n",
      "an outstanding technique for program analysis in the near future.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6352\n",
      "Abstract:   We are in the dawn of deep learning explosion for smartphones. To bridge the\n",
      "gap between research and practice, we present the first empirical study on\n",
      "16,500 the most popular Android apps, demystifying how smartphone apps exploit\n",
      "deep learning in the wild. To this end, we build a new static tool that\n",
      "dissects apps and analyzes their deep learning functions. Our study answers\n",
      "threefold questions: what are the early adopter apps of deep learning, what do\n",
      "they use deep learning for, and how do their deep learning models look like.\n",
      "Our study has strong implications for app developers, smartphone vendors, and\n",
      "deep learning R\\&D. On one hand, our findings paint a promising picture of deep\n",
      "learning for smartphones, showing the prosperity of mobile deep learning\n",
      "frameworks as well as the prosperity of apps building their cores atop deep\n",
      "learning. On the other hand, our findings urge optimizations on deep learning\n",
      "models deployed on smartphones, the protection of these models, and validation\n",
      "of research ideas on these models.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6335\n",
      "Abstract:   Deep learning has emerged as a powerful machine learning technique that\n",
      "learns multiple layers of representations or features of the data and produces\n",
      "state-of-the-art prediction results. Along with the success of deep learning in\n",
      "many other application domains, deep learning is also popularly used in\n",
      "sentiment analysis in recent years. This paper first gives an overview of deep\n",
      "learning and then provides a comprehensive survey of its current applications\n",
      "in sentiment analysis.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6203\n",
      "Abstract:   Deep learning has been extensively applied in many optical imaging\n",
      "applications in recent years. Despite the success, the limitations and\n",
      "drawbacks of deep learning in optical imaging have been seldom investigated. In\n",
      "this work, we show that conventional linear-regression-based methods can\n",
      "outperform the previously proposed deep learning approaches for two black-box\n",
      "optical imaging problems in some extent. Deep learning demonstrates its\n",
      "weakness especially when the number of training samples is small. The\n",
      "advantages and disadvantages of linear-regression-based methods and deep\n",
      "learning are analyzed and compared. Since many optical systems are essentially\n",
      "linear, a deep learning network containing many nonlinearity functions\n",
      "sometimes may not be the most suitable option.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6048\n",
      "Abstract:   Deep learning has arguably achieved tremendous success in recent years. In\n",
      "simple words, deep learning uses the composition of many nonlinear functions to\n",
      "model the complex dependency between input features and labels. While neural\n",
      "networks have a long history, recent advances have greatly improved their\n",
      "performance in computer vision, natural language processing, etc. From the\n",
      "statistical and scientific perspective, it is natural to ask: What is deep\n",
      "learning? What are the new characteristics of deep learning, compared with\n",
      "classical methods? What are the theoretical foundations of deep learning? To\n",
      "answer these questions, we introduce common neural network models (e.g.,\n",
      "convolutional neural nets, recurrent neural nets, generative adversarial nets)\n",
      "and training techniques (e.g., stochastic gradient descent, dropout, batch\n",
      "normalization) from a statistical point of view. Along the way, we highlight\n",
      "new characteristics of deep learning (including depth and over-parametrization)\n",
      "and explain their practical and theoretical benefits. We also sample recent\n",
      "results on theories of deep learning, many of which are only suggestive. While\n",
      "a complete understanding of deep learning remains elusive, we hope that our\n",
      "perspectives and discussions serve as a stimulus for new statistical research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6017\n",
      "Abstract:   The great success of deep learning poses urgent challenges for understanding\n",
      "its working mechanism and rationality. The depth, structure, and massive size\n",
      "of the data are recognized to be three key ingredients for deep learning. Most\n",
      "of the recent theoretical studies for deep learning focus on the necessity and\n",
      "advantages of depth and structures of neural networks. In this paper, we aim at\n",
      "rigorous verification of the importance of massive data in embodying the\n",
      "out-performance of deep learning. To approximate and learn spatially sparse and\n",
      "smooth functions, we establish a novel sampling theorem in learning theory to\n",
      "show the necessity of massive data. We then prove that implementing the\n",
      "classical empirical risk minimization on some deep nets facilitates in\n",
      "realization of the optimal learning rates derived in the sampling theorem. This\n",
      "perhaps explains why deep learning performs so well in the era of big data.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input query\n",
    "query = \"Deep learning is\"\n",
    "query_vec = vectorizer_loaded.transform([query])\n",
    "\n",
    "# Compute similarities\n",
    "similarities = cosine_similarity(query_vec, tfidf_matrix_loaded).flatten()\n",
    "\n",
    "# Sort and display results\n",
    "sorted_indices = similarities.argsort()[::-1]\n",
    "for idx in sorted_indices[:10]:  # Top 10 results\n",
    "    print(f\"Similarity Score: {similarities[idx]:.4f}\")\n",
    "    print(f\"Abstract: {df['abstract'].iloc[idx]}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c43268-eff6-4c92-8a60-0cf822122e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
