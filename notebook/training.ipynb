{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718dcad0-6a51-4342-a758-3c092bb4b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2787310-9f6b-48af-ab1c-3469ecd6ba70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entanglement and control operations in Ising i...</td>\n",
       "      <td>Entanglement generated by Ising model has be...</td>\n",
       "      <td>Francisco Delgado (Tecnologico de Monterrey, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronal lines and dust formation in SN 2005ip:...</td>\n",
       "      <td>We present optical photometry and spectrosco...</td>\n",
       "      <td>Nathan Smith, Jeffrey M. Silverman, Ryan Chorn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Products of straight spaces</td>\n",
       "      <td>A metric space X is straight if for each fin...</td>\n",
       "      <td>Alessandro Berarducci, Dikran Dikranjan, Jan P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Carina Nebula: A Laboratory for Feedback a...</td>\n",
       "      <td>The Carina Nebula (NGC 3372) is our richest ...</td>\n",
       "      <td>Nathan Smith and Kate J. Brooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metric groups attached to biextensions</td>\n",
       "      <td>Let $G$ be a connnected, unipotent, perfect ...</td>\n",
       "      <td>Swarnendu Datta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Entanglement and control operations in Ising i...   \n",
       "1  Coronal lines and dust formation in SN 2005ip:...   \n",
       "2                        Products of straight spaces   \n",
       "3  The Carina Nebula: A Laboratory for Feedback a...   \n",
       "4             Metric groups attached to biextensions   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    Entanglement generated by Ising model has be...   \n",
       "1    We present optical photometry and spectrosco...   \n",
       "2    A metric space X is straight if for each fin...   \n",
       "3    The Carina Nebula (NGC 3372) is our richest ...   \n",
       "4    Let $G$ be a connnected, unipotent, perfect ...   \n",
       "\n",
       "                                             authors  \n",
       "0  Francisco Delgado (Tecnologico de Monterrey, C...  \n",
       "1  Nathan Smith, Jeffrey M. Silverman, Ryan Chorn...  \n",
       "2  Alessandro Berarducci, Dikran Dikranjan, Jan P...  \n",
       "3                    Nathan Smith and Kate J. Brooks  \n",
       "4                                    Swarnendu Datta  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', 'dataset.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9768e110-40c4-46d5-a58e-fe0e757201a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1256074, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464ef8d3-3b09-4d12-a8a2-133381bfc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df['abstract_cleaned'] = df['abstract'].str.lower()\n",
    "\n",
    "# Convert abstracts to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['abstract_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9667484f-b97f-44e2-82aa-e7c8454a2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstracts with similarity score > 0.5:\n",
      "\n",
      "Similarity Score: 0.7577\n",
      "Abstract:   Deep learning has achieved a great success in many areas, from computer\n",
      "vision to natural language processing, to game playing, and much more. Yet,\n",
      "what deep learning is really doing is still an open question. There are a lot\n",
      "of works in this direction. For example, [5] tried to explain deep learning by\n",
      "group renormalization, and [6] tried to explain deep learning from the view of\n",
      "functional approximation. In order to address this very crucial question, here\n",
      "we see deep learning from perspective of mechanical learning and learning\n",
      "machine (see [1], [2]). From this particular angle, we can see deep learning\n",
      "much better and answer with confidence: What deep learning is really doing? why\n",
      "it works well, how it works, and how much data is necessary for learning. We\n",
      "also will discuss advantages and disadvantages of deep learning at the end of\n",
      "this work.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.7360\n",
      "Abstract:   The great success of deep learning shows that its technology contains\n",
      "profound truth, and understanding its internal mechanism not only has important\n",
      "implications for the development of its technology and effective application in\n",
      "various fields, but also provides meaningful insights into the understanding of\n",
      "human brain mechanism. At present, most of the theoretical research on deep\n",
      "learning is based on mathematics. This dissertation proposes that the neural\n",
      "network of deep learning is a physical system, examines deep learning from\n",
      "three different perspectives: microscopic, macroscopic, and physical world\n",
      "views, answers multiple theoretical puzzles in deep learning by using physics\n",
      "principles. For example, from the perspective of quantum mechanics and\n",
      "statistical physics, this dissertation presents the calculation methods for\n",
      "convolution calculation, pooling, normalization, and Restricted Boltzmann\n",
      "Machine, as well as the selection of cost functions, explains why deep learning\n",
      "must be deep, what characteristics are learned in deep learning, why\n",
      "Convolutional Neural Networks do not have to be trained layer by layer, and the\n",
      "limitations of deep learning, etc., and proposes the theoretical direction and\n",
      "basis for the further development of deep learning now and in the future. The\n",
      "brilliance of physics flashes in deep learning, we try to establish the deep\n",
      "learning technology based on the scientific theory of physics.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6627\n",
      "Abstract:   Deep Learning is one of the newest trends in Machine Learning and Artificial\n",
      "Intelligence research. It is also one of the most popular scientific research\n",
      "trends now-a-days. Deep learning methods have brought revolutionary advances in\n",
      "computer vision and machine learning. Every now and then, new and new deep\n",
      "learning techniques are being born, outperforming state-of-the-art machine\n",
      "learning and even existing deep learning techniques. In recent years, the world\n",
      "has seen many major breakthroughs in this field. Since deep learning is\n",
      "evolving at a huge speed, its kind of hard to keep track of the regular\n",
      "advances especially for new researchers. In this paper, we are going to briefly\n",
      "discuss about recent advances in Deep Learning for past few years.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6404\n",
      "Abstract:   Over the past few years, deep learning has risen to the foreground as a topic\n",
      "of massive interest, mainly as a result of successes obtained in solving\n",
      "large-scale image processing tasks. There are multiple challenging mathematical\n",
      "problems involved in applying deep learning: most deep learning methods require\n",
      "the solution of hard optimisation problems, and a good understanding of the\n",
      "tradeoff between computational effort, amount of data and model complexity is\n",
      "required to successfully design a deep learning approach for a given problem. A\n",
      "large amount of progress made in deep learning has been based on heuristic\n",
      "explorations, but there is a growing effort to mathematically understand the\n",
      "structure in existing deep learning methods and to systematically design new\n",
      "deep learning methods to preserve certain types of structure in deep learning.\n",
      "In this article, we review a number of these directions: some deep neural\n",
      "networks can be understood as discretisations of dynamical systems, neural\n",
      "networks can be designed to have desirable properties such as invertibility or\n",
      "group equivariance, and new algorithmic frameworks based on conformal\n",
      "Hamiltonian systems and Riemannian manifolds to solve the optimisation problems\n",
      "have been proposed. We conclude our review of each of these topics by\n",
      "discussing some open problems that we consider to be interesting directions for\n",
      "future research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6379\n",
      "Abstract:   Deep learning has made significant breakthroughs in various fields of\n",
      "artificial intelligence. Advantages of deep learning include the ability to\n",
      "capture highly complicated features, weak involvement of human engineering,\n",
      "etc. However, it is still virtually impossible to use deep learning to analyze\n",
      "programs since deep architectures cannot be trained effectively with pure back\n",
      "propagation. In this pioneering paper, we propose the \"coding criterion\" to\n",
      "build program vector representations, which are the premise of deep learning\n",
      "for program analysis. Our representation learning approach directly makes deep\n",
      "learning a reality in this new field. We evaluate the learned vector\n",
      "representations both qualitatively and quantitatively. We conclude, based on\n",
      "the experiments, the coding criterion is successful in building program\n",
      "representations. To evaluate whether deep learning is beneficial for program\n",
      "analysis, we feed the representations to deep neural networks, and achieve\n",
      "higher accuracy in the program classification task than \"shallow\" methods, such\n",
      "as logistic regression and the support vector machine. This result confirms the\n",
      "feasibility of deep learning to analyze programs. It also gives primary\n",
      "evidence of its success in this new field. We believe deep learning will become\n",
      "an outstanding technique for program analysis in the near future.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6352\n",
      "Abstract:   We are in the dawn of deep learning explosion for smartphones. To bridge the\n",
      "gap between research and practice, we present the first empirical study on\n",
      "16,500 the most popular Android apps, demystifying how smartphone apps exploit\n",
      "deep learning in the wild. To this end, we build a new static tool that\n",
      "dissects apps and analyzes their deep learning functions. Our study answers\n",
      "threefold questions: what are the early adopter apps of deep learning, what do\n",
      "they use deep learning for, and how do their deep learning models look like.\n",
      "Our study has strong implications for app developers, smartphone vendors, and\n",
      "deep learning R\\&D. On one hand, our findings paint a promising picture of deep\n",
      "learning for smartphones, showing the prosperity of mobile deep learning\n",
      "frameworks as well as the prosperity of apps building their cores atop deep\n",
      "learning. On the other hand, our findings urge optimizations on deep learning\n",
      "models deployed on smartphones, the protection of these models, and validation\n",
      "of research ideas on these models.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6335\n",
      "Abstract:   Deep learning has emerged as a powerful machine learning technique that\n",
      "learns multiple layers of representations or features of the data and produces\n",
      "state-of-the-art prediction results. Along with the success of deep learning in\n",
      "many other application domains, deep learning is also popularly used in\n",
      "sentiment analysis in recent years. This paper first gives an overview of deep\n",
      "learning and then provides a comprehensive survey of its current applications\n",
      "in sentiment analysis.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6203\n",
      "Abstract:   Deep learning has been extensively applied in many optical imaging\n",
      "applications in recent years. Despite the success, the limitations and\n",
      "drawbacks of deep learning in optical imaging have been seldom investigated. In\n",
      "this work, we show that conventional linear-regression-based methods can\n",
      "outperform the previously proposed deep learning approaches for two black-box\n",
      "optical imaging problems in some extent. Deep learning demonstrates its\n",
      "weakness especially when the number of training samples is small. The\n",
      "advantages and disadvantages of linear-regression-based methods and deep\n",
      "learning are analyzed and compared. Since many optical systems are essentially\n",
      "linear, a deep learning network containing many nonlinearity functions\n",
      "sometimes may not be the most suitable option.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6048\n",
      "Abstract:   Deep learning has arguably achieved tremendous success in recent years. In\n",
      "simple words, deep learning uses the composition of many nonlinear functions to\n",
      "model the complex dependency between input features and labels. While neural\n",
      "networks have a long history, recent advances have greatly improved their\n",
      "performance in computer vision, natural language processing, etc. From the\n",
      "statistical and scientific perspective, it is natural to ask: What is deep\n",
      "learning? What are the new characteristics of deep learning, compared with\n",
      "classical methods? What are the theoretical foundations of deep learning? To\n",
      "answer these questions, we introduce common neural network models (e.g.,\n",
      "convolutional neural nets, recurrent neural nets, generative adversarial nets)\n",
      "and training techniques (e.g., stochastic gradient descent, dropout, batch\n",
      "normalization) from a statistical point of view. Along the way, we highlight\n",
      "new characteristics of deep learning (including depth and over-parametrization)\n",
      "and explain their practical and theoretical benefits. We also sample recent\n",
      "results on theories of deep learning, many of which are only suggestive. While\n",
      "a complete understanding of deep learning remains elusive, we hope that our\n",
      "perspectives and discussions serve as a stimulus for new statistical research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6017\n",
      "Abstract:   The great success of deep learning poses urgent challenges for understanding\n",
      "its working mechanism and rationality. The depth, structure, and massive size\n",
      "of the data are recognized to be three key ingredients for deep learning. Most\n",
      "of the recent theoretical studies for deep learning focus on the necessity and\n",
      "advantages of depth and structures of neural networks. In this paper, we aim at\n",
      "rigorous verification of the importance of massive data in embodying the\n",
      "out-performance of deep learning. To approximate and learn spatially sparse and\n",
      "smooth functions, we establish a novel sampling theorem in learning theory to\n",
      "show the necessity of massive data. We then prove that implementing the\n",
      "classical empirical risk minimization on some deep nets facilitates in\n",
      "realization of the optimal learning rates derived in the sampling theorem. This\n",
      "perhaps explains why deep learning performs so well in the era of big data.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.6008\n",
      "Abstract:   This paper presents a basic property of region dividing of ReLU (rectified\n",
      "linear unit) deep learning when new layers are successively added, by which two\n",
      "new perspectives of interpreting deep learning are given. The first is related\n",
      "to decision trees and forests; we construct a deep learning structure\n",
      "equivalent to a forest in classification abilities, which means that certain\n",
      "kinds of ReLU deep learning can be considered as forests. The second\n",
      "perspective is that Haar wavelet represented functions can be approximated by\n",
      "ReLU deep learning with arbitrary precision; and then a general conclusion of\n",
      "function approximation abilities of ReLU deep learning is given. Finally,\n",
      "generalize some of the conclusions of ReLU deep learning to the case of\n",
      "sigmoid-unit deep learning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5971\n",
      "Abstract:   The Intel Collaborative Research Institute for Computational Intelligence\n",
      "(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\n",
      "research from its foundation in 2012. We have asked six leading ICRI-CI Deep\n",
      "Learning researchers to address the challenge of \"Why & When Deep Learning\n",
      "works\", with the goal of looking inside Deep Learning, providing insights on\n",
      "how deep networks function, and uncovering key observations on their\n",
      "expressiveness, limitations, and potential. The output of this challenge\n",
      "resulted in five papers that address different facets of deep learning. These\n",
      "different facets include a high-level understating of why and when deep\n",
      "networks work (and do not work), the impact of geometry on the expressiveness\n",
      "of deep networks, and making deep networks interpretable.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5939\n",
      "Abstract:   While deep learning has resulted in major breakthroughs in many application\n",
      "domains, the frameworks commonly used in deep learning remain fragile to\n",
      "artificially-crafted and imperceptible changes in the data. In response to this\n",
      "fragility, adversarial training has emerged as a principled approach for\n",
      "enhancing the robustness of deep learning with respect to norm-bounded\n",
      "perturbations. However, there are other sources of fragility for deep learning\n",
      "that are arguably more common and less thoroughly studied. Indeed, natural\n",
      "variation such as lighting or weather conditions can significantly degrade the\n",
      "accuracy of trained neural networks, proving that such natural variation\n",
      "presents a significant challenge for deep learning.\n",
      "  In this paper, we propose a paradigm shift from perturbation-based\n",
      "adversarial robustness toward model-based robust deep learning. Our objective\n",
      "is to provide general training algorithms that can be used to train deep neural\n",
      "networks to be robust against natural variation in data. Critical to our\n",
      "paradigm is first obtaining a model of natural variation which can be used to\n",
      "vary data over a range of natural conditions. Such models may be either known a\n",
      "priori or else learned from data. In the latter case, we show that deep\n",
      "generative models can be used to learn models of natural variation that are\n",
      "consistent with realistic conditions. We then exploit such models in three\n",
      "novel model-based robust training algorithms in order to enhance the robustness\n",
      "of deep learning with respect to the given model. Our extensive experiments\n",
      "show that across a variety of naturally-occurring conditions and across various\n",
      "datasets, deep neural networks trained with our model-based algorithms\n",
      "significantly outperform both standard deep learning algorithms as well as\n",
      "norm-bounded robust deep learning algorithms.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5915\n",
      "Abstract:   Deep neural networks demonstrated their ability to provide remarkable\n",
      "performances on a wide range of supervised learning tasks (e.g., image\n",
      "classification) when trained on extensive collections of labeled data (e.g.,\n",
      "ImageNet). However, creating such large datasets requires a considerable amount\n",
      "of resources, time, and effort. Such resources may not be available in many\n",
      "practical cases, limiting the adoption and the application of many deep\n",
      "learning methods. In a search for more data-efficient deep learning methods to\n",
      "overcome the need for large annotated datasets, there is a rising research\n",
      "interest in semi-supervised learning and its applications to deep neural\n",
      "networks to reduce the amount of labeled data required, by either developing\n",
      "novel methods or adopting existing semi-supervised learning frameworks for a\n",
      "deep learning setting. In this paper, we provide a comprehensive overview of\n",
      "deep semi-supervised learning, starting with an introduction to the field,\n",
      "followed by a summarization of the dominant semi-supervised approaches in deep\n",
      "learning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5879\n",
      "Abstract:   Deep learning has emerged as a strong and efficient framework that can be\n",
      "applied to a broad spectrum of complex learning problems which were difficult\n",
      "to solve using the traditional machine learning techniques in the past. In the\n",
      "last few years, deep learning has advanced radically in such a way that it can\n",
      "surpass human-level performance on a number of tasks. As a consequence, deep\n",
      "learning is being extensively used in most of the recent day-to-day\n",
      "applications. However, security of deep learning systems are vulnerable to\n",
      "crafted adversarial examples, which may be imperceptible to the human eye, but\n",
      "can lead the model to misclassify the output. In recent times, different types\n",
      "of adversaries based on their threat model leverage these vulnerabilities to\n",
      "compromise a deep learning system where adversaries have high incentives.\n",
      "Hence, it is extremely important to provide robustness to deep learning\n",
      "algorithms against these adversaries. However, there are only a few strong\n",
      "countermeasures which can be used in all types of attack scenarios to design a\n",
      "robust deep learning system. In this paper, we attempt to provide a detailed\n",
      "discussion on different types of adversarial attacks with various threat models\n",
      "and also elaborate the efficiency and challenges of recent countermeasures\n",
      "against them.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5849\n",
      "Abstract:   Inspired by the success of deploying deep learning in the fields of Computer\n",
      "Vision and Natural Language Processing, this learning paradigm has also found\n",
      "its way into the field of Music Information Retrieval. In order to benefit from\n",
      "deep learning in an effective, but also efficient manner, deep transfer\n",
      "learning has become a common approach. In this approach, it is possible to\n",
      "reuse the output of a pre-trained neural network as the basis for a new\n",
      "learning task. The underlying hypothesis is that if the initial and new\n",
      "learning tasks show commonalities and are applied to the same type of input\n",
      "data (e.g. music audio), the generated deep representation of the data is also\n",
      "informative for the new task. Since, however, most of the networks used to\n",
      "generate deep representations are trained using a single initial learning\n",
      "source, their representation is unlikely to be informative for all possible\n",
      "future tasks. In this paper, we present the results of our investigation of\n",
      "what are the most important factors to generate deep representations for the\n",
      "data and learning tasks in the music domain. We conducted this investigation\n",
      "via an extensive empirical study that involves multiple learning sources, as\n",
      "well as multiple deep learning architectures with varying levels of information\n",
      "sharing between sources, in order to learn music representations. We then\n",
      "validate these representations considering multiple target datasets for\n",
      "evaluation. The results of our experiments yield several insights on how to\n",
      "approach the design of methods for learning widely deployable deep data\n",
      "representations in the music domain.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5808\n",
      "Abstract:   Even though active learning forms an important pillar of machine learning,\n",
      "deep learning tools are not prevalent within it. Deep learning poses several\n",
      "difficulties when used in an active learning setting. First, active learning\n",
      "(AL) methods generally rely on being able to learn and update models from small\n",
      "amounts of data. Recent advances in deep learning, on the other hand, are\n",
      "notorious for their dependence on large amounts of data. Second, many AL\n",
      "acquisition functions rely on model uncertainty, yet deep learning methods\n",
      "rarely represent such model uncertainty. In this paper we combine recent\n",
      "advances in Bayesian deep learning into the active learning framework in a\n",
      "practical way. We develop an active learning framework for high dimensional\n",
      "data, a task which has been extremely challenging so far, with very sparse\n",
      "existing literature. Taking advantage of specialised models such as Bayesian\n",
      "convolutional neural networks, we demonstrate our active learning techniques\n",
      "with image data, obtaining a significant improvement on existing active\n",
      "learning approaches. We demonstrate this on both the MNIST dataset, as well as\n",
      "for skin cancer diagnosis from lesion images (ISIC2016 task).\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5788\n",
      "Abstract:   Deep learning has recently become one of the most popular sub-fields of\n",
      "machine learning owing to its distributed data representation with multiple\n",
      "levels of abstraction. A diverse range of deep learning algorithms are being\n",
      "employed to solve conventional artificial intelligence problems. This paper\n",
      "gives an overview of some of the most widely used deep learning algorithms\n",
      "applied in the field of computer vision. It first inspects the various\n",
      "approaches of deep learning algorithms, followed by a description of their\n",
      "applications in image classification, object identification, image extraction\n",
      "and semantic segmentation in the presence of noise. The paper concludes with\n",
      "the discussion of the future scope and challenges for construction and training\n",
      "of deep neural networks.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5784\n",
      "Abstract:   In this paper, we present a statistical-mechanical analysis of deep learning.\n",
      "We elucidate some of the essential components of deep learning---pre-training\n",
      "by unsupervised learning and fine tuning by supervised learning. We formulate\n",
      "the extraction of features from the training data as a margin criterion in a\n",
      "high-dimensional feature-vector space. The self-organized classifier is then\n",
      "supplied with small amounts of labelled data, as in deep learning. Although we\n",
      "employ a simple single-layer perceptron model, rather than directly analyzing a\n",
      "multi-layer neural network, we find a nontrivial phase transition that is\n",
      "dependent on the number of unlabelled data in the generalization error of the\n",
      "resultant classifier. In this sense, we evaluate the efficacy of the\n",
      "unsupervised learning component of deep learning. The analysis is performed by\n",
      "the replica method, which is a sophisticated tool in statistical mechanics. We\n",
      "validate our result in the manner of deep learning, using a simple iterative\n",
      "algorithm to learn the weight vector on the basis of belief propagation.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5766\n",
      "Abstract:   Recently, several JavaScript-based deep learning frameworks have emerged,\n",
      "making it possible to perform deep learning tasks directly in browsers.\n",
      "However, little is known on what and how well we can do with these frameworks\n",
      "for deep learning in browsers. To bridge the knowledge gap, in this paper, we\n",
      "conduct the first empirical study of deep learning in browsers. We survey 7\n",
      "most popular JavaScript-based deep learning frameworks, investigating to what\n",
      "extent deep learning tasks have been supported in browsers so far. Then we\n",
      "measure the performance of different frameworks when running different deep\n",
      "learning tasks. Finally, we dig out the performance gap between deep learning\n",
      "in browsers and on native platforms by comparing the performance of\n",
      "TensorFlow.js and TensorFlow in Python. Our findings could help application\n",
      "developers, deep-learning framework vendors and browser vendors to improve the\n",
      "efficiency of deep learning in browsers.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5760\n",
      "Abstract:   In the recent time deep learning has achieved huge popularity due to its\n",
      "performance in various machine learning algorithms. Deep learning as\n",
      "hierarchical or structured learning attempts to model high level abstractions\n",
      "in data by using a group of processing layers. The foundation of deep learning\n",
      "architectures is inspired by the understanding of information processing and\n",
      "neural responses in human brain. The architectures are created by stacking\n",
      "multiple linear or non-linear operations. The article mainly focuses on the\n",
      "state-of-art deep learning models and various real world applications specific\n",
      "training methods. Selecting optimal architecture for specific problem is a\n",
      "challenging task, at a closing stage of the article we proposed optimal\n",
      "approach to deep convolutional architecture for the application of image\n",
      "recognition.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5740\n",
      "Abstract:   Recent years, deep learning is increasingly prevalent in the field of\n",
      "Software Engineering (SE). However, many open issues still remain to be\n",
      "investigated. How do researchers integrate deep learning into SE problems?\n",
      "Which SE phases are facilitated by deep learning? Do practitioners benefit from\n",
      "deep learning? The answers help practitioners and researchers develop practical\n",
      "deep learning models for SE tasks. To answer these questions, we conduct a\n",
      "bibliography analysis on 98 research papers in SE that use deep learning\n",
      "techniques. We find that 41 SE tasks in all SE phases have been facilitated by\n",
      "deep learning integrated solutions. In which, 84.7% papers only use standard\n",
      "deep learning models and their variants to solve SE problems. The\n",
      "practicability becomes a concern in utilizing deep learning techniques. How to\n",
      "improve the effectiveness, efficiency, understandability, and testability of\n",
      "deep learning based solutions may attract more SE researchers in the future.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5732\n",
      "Abstract:   Deep learning is a branch of artificial intelligence where networks of simple\n",
      "interconnected units are used to extract patterns from data in order to solve\n",
      "complex problems. Deep learning algorithms have shown groundbreaking\n",
      "performance in a variety of sophisticated tasks, especially those related to\n",
      "images. They have often matched or exceeded human performance. Since the\n",
      "medical field of radiology mostly relies on extracting useful information from\n",
      "images, it is a very natural application area for deep learning, and research\n",
      "in this area has rapidly grown in recent years. In this article, we review the\n",
      "clinical reality of radiology and discuss the opportunities for application of\n",
      "deep learning algorithms. We also introduce basic concepts of deep learning\n",
      "including convolutional neural networks. Then, we present a survey of the\n",
      "research in deep learning applied to radiology. We organize the studies by the\n",
      "types of specific tasks that they attempt to solve and review the broad range\n",
      "of utilized deep learning algorithms. Finally, we briefly discuss opportunities\n",
      "and challenges for incorporating deep learning in the radiology practice of the\n",
      "future.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5728\n",
      "Abstract:   Deep Q-Learning is an important reinforcement learning algorithm, which\n",
      "involves training a deep neural network, called Deep Q-Network (DQN), to\n",
      "approximate the well-known Q-function. Although wildly successful under\n",
      "laboratory conditions, serious gaps between theory and practice as well as a\n",
      "lack of formal guarantees prevent its use in the real world. Adopting a\n",
      "dynamical systems perspective, we provide a theoretical analysis of a popular\n",
      "version of Deep Q-Learning under realistic and verifiable assumptions. More\n",
      "specifically, we prove an important result on the convergence of the algorithm,\n",
      "characterizing the asymptotic behavior of the learning process. Our result\n",
      "sheds light on hitherto unexplained properties of the algorithm and helps\n",
      "understand empirical observations, such as performance inconsistencies even\n",
      "after training. Unlike previous theories, our analysis accommodates state\n",
      "Markov processes with multiple stationary distributions. In spite of the focus\n",
      "on Deep Q-Learning, we believe that our theory may be applied to understand\n",
      "other deep learning algorithms\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5722\n",
      "Abstract:   Deep learning has allowed a paradigm shift in pattern recognition, from using\n",
      "hand-crafted features together with statistical classifiers to using\n",
      "general-purpose learning procedures for learning data-driven representations,\n",
      "features, and classifiers together. The application of this new paradigm has\n",
      "been particularly successful in computer vision, in which the development of\n",
      "deep learning methods for vision applications has become a hot research topic.\n",
      "Given that deep learning has already attracted the attention of the robot\n",
      "vision community, the main purpose of this survey is to address the use of deep\n",
      "learning in robot vision. To achieve this, a comprehensive overview of deep\n",
      "learning and its usage in computer vision is given, that includes a description\n",
      "of the most frequently used neural models and their main application areas.\n",
      "Then, the standard methodology and tools used for designing deep-learning based\n",
      "vision systems are presented. Afterwards, a review of the principal work using\n",
      "deep learning in robot vision is presented, as well as current and future\n",
      "trends related to the use of deep learning in robotics. This survey is intended\n",
      "to be a guide for the developers of robot vision systems.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5704\n",
      "Abstract:   Deep learning is very effective at jointly learning feature representations\n",
      "and classification models, especially when dealing with high dimensional input\n",
      "patterns. Probabilistic logic reasoning, on the other hand, is capable to take\n",
      "consistent and robust decisions in complex environments. The integration of\n",
      "deep learning and logic reasoning is still an open-research problem and it is\n",
      "considered to be the key for the development of real intelligent agents. This\n",
      "paper presents Deep Logic Models, which are deep graphical models integrating\n",
      "deep learning and logic reasoning both for learning and inference. Deep Logic\n",
      "Models create an end-to-end differentiable architecture, where deep learners\n",
      "are embedded into a network implementing a continuous relaxation of the logic\n",
      "knowledge. The learning process allows to jointly learn the weights of the deep\n",
      "learners and the meta-parameters controlling the high-level reasoning. The\n",
      "experimental results show that the proposed methodology overtakes the\n",
      "limitations of the other approaches that have been proposed to bridge deep\n",
      "learning and reasoning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5671\n",
      "Abstract:   Like any field of empirical science, AI may be approached axiomatically. We\n",
      "formulate requirements for a general-purpose, human-level AI system in terms of\n",
      "postulates. We review the methodology of deep learning, examining the explicit\n",
      "and tacit assumptions in deep learning research. Deep Learning methodology\n",
      "seeks to overcome limitations in traditional machine learning research as it\n",
      "combines facets of model richness, generality, and practical applicability. The\n",
      "methodology so far has produced outstanding results due to a productive synergy\n",
      "of function approximation, under plausible assumptions of irreducibility and\n",
      "the efficiency of back-propagation family of algorithms. We examine these\n",
      "winning traits of deep learning, and also observe the various known failure\n",
      "modes of deep learning. We conclude by giving recommendations on how to extend\n",
      "deep learning methodology to cover the postulates of general-purpose AI\n",
      "including modularity, and cognitive architecture. We also relate deep learning\n",
      "to advances in theoretical neuroscience research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5627\n",
      "Abstract:   Deep learning is increasingly being used in high-stake decision making\n",
      "applications that affect individual lives. However, deep learning models might\n",
      "exhibit algorithmic discrimination behaviors with respect to protected groups,\n",
      "potentially posing negative impacts on individuals and society. Therefore,\n",
      "fairness in deep learning has attracted tremendous attention recently. We\n",
      "provide a review covering recent progresses to tackle algorithmic fairness\n",
      "problems of deep learning from the computational perspective. Specifically, we\n",
      "show that interpretability can serve as a useful ingredient to diagnose the\n",
      "reasons that lead to algorithmic discrimination. We also discuss fairness\n",
      "mitigation approaches categorized according to three stages of deep learning\n",
      "life-cycle, aiming to push forward the area of fairness in deep learning and\n",
      "build genuinely fair and reliable deep learning systems.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5616\n",
      "Abstract:   Should we input known genome sequence features or input sequence itself in\n",
      "deep learning framework? As deep learning more popular in various applications,\n",
      "researchers often come to question whether to generate features or use raw\n",
      "sequences for deep learning. To answer this question, we study the prediction\n",
      "accuracy of precursor miRNA prediction of feature-based deep belief network and\n",
      "sequence-based convolution neural network. Tested on a variant of six-layer\n",
      "convolution neural net and three-layer deep belief network, we find the raw\n",
      "sequence input based convolution neural network model performs similar or\n",
      "slightly better than feature based deep belief networks with best accuracy\n",
      "values of 0.995 and 0.990, respectively. Both the models outperform existing\n",
      "benchmarks models. The results shows us that if provided large enough data,\n",
      "well devised raw sequence based deep learning models can replace feature based\n",
      "deep learning models. However, construction of well behaved deep learning model\n",
      "can be very challenging. In cased features can be easily extracted,\n",
      "feature-based deep learning models may be a better alternative.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5590\n",
      "Abstract:   This paper introduces a novel measure-theoretic theory for machine learning\n",
      "that does not require statistical assumptions. Based on this theory, a new\n",
      "regularization method in deep learning is derived and shown to outperform\n",
      "previous methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed\n",
      "theory provides a theoretical basis for a family of practically successful\n",
      "regularization methods in deep learning. We discuss several consequences of our\n",
      "results on one-shot learning, representation learning, deep learning, and\n",
      "curriculum learning. Unlike statistical learning theory, the proposed learning\n",
      "theory analyzes each problem instance individually via measure theory, rather\n",
      "than a set of problem instances via statistics. As a result, it provides\n",
      "different types of results and insights when compared to statistical learning\n",
      "theory.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5576\n",
      "Abstract:   The rise of social media is enabling people to freely express their opinions\n",
      "about products and services. The aim of sentiment analysis is to automatically\n",
      "determine subject's sentiment (e.g., positive, negative, or neutral) towards a\n",
      "particular aspect such as topic, product, movie, news etc. Deep learning has\n",
      "recently emerged as a powerful machine learning technique to tackle a growing\n",
      "demand of accurate sentiment analysis. However, limited work has been conducted\n",
      "to apply deep learning algorithms to languages other than English, such as\n",
      "Persian. In this work, two deep learning models (deep autoencoders and deep\n",
      "convolutional neural networks (CNNs)) are developed and applied to a novel\n",
      "Persian movie reviews dataset. The proposed deep learning models are analyzed\n",
      "and compared with the state-of-the-art shallow multilayer perceptron (MLP)\n",
      "based machine learning model. Simulation results demonstrate the enhanced\n",
      "performance of deep learning over state-of-the-art MLP.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5561\n",
      "Abstract:   Deep learning techniques have received much attention in the area of image\n",
      "denoising. However, there are substantial differences in the various types of\n",
      "deep learning methods dealing with image denoising. Specifically,\n",
      "discriminative learning based on deep learning can ably address the issue of\n",
      "Gaussian noise. Optimization models based on deep learning are effective in\n",
      "estimating the real noise. However, there has thus far been little related\n",
      "research to summarize the different deep learning techniques for image\n",
      "denoising. In this paper, we offer a comparative study of deep techniques in\n",
      "image denoising. We first classify the deep convolutional neural networks\n",
      "(CNNs) for additive white noisy images; the deep CNNs for real noisy images;\n",
      "the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images,\n",
      "which represents the combination of noisy, blurred and low-resolution images.\n",
      "Then, we analyze the motivations and principles of the different types of deep\n",
      "learning methods. Next, we compare the state-of-the-art methods on public\n",
      "denoising datasets in terms of quantitative and qualitative analysis. Finally,\n",
      "we point out some potential challenges and directions of future research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5556\n",
      "Abstract:   Deep reinforcement learning is revolutionizing the artificial intelligence\n",
      "field. Currently, it serves as a good starting point for constructing\n",
      "intelligent autonomous systems which offer a better knowledge of the visual\n",
      "world. It is possible to scale deep reinforcement learning with the use of deep\n",
      "learning and do amazing tasks such as use of pixels in playing video games. In\n",
      "this paper, key concepts of deep reinforcement learning including reward\n",
      "function, differences between reinforcement learning and supervised learning\n",
      "and models for implementation of reinforcement are discussed. Key challenges\n",
      "related to the implementation of reinforcement learning in conversational AI\n",
      "domain are identified as well as discussed in detail. Various conversational\n",
      "models which are based on deep reinforcement learning (as well as deep\n",
      "learning) are also discussed. In summary, this paper discusses key aspects of\n",
      "deep reinforcement learning which are crucial for designing an efficient\n",
      "conversational AI.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5498\n",
      "Abstract:   Deep Learning has become one of the primary research areas in developing\n",
      "intelligent machines. Most of the well-known applications (such as Speech\n",
      "Recognition, Image Processing and NLP) of AI are driven by Deep Learning. Deep\n",
      "Learning algorithms mimic human brain using artificial neural networks and\n",
      "progressively learn to accurately solve a given problem. But there are\n",
      "significant challenges in Deep Learning systems. There have been many attempts\n",
      "to make deep learning models imitate the biological neural network. However,\n",
      "many deep learning models have performed poorly in the presence of adversarial\n",
      "examples. Poor performance in adversarial examples leads to adversarial attacks\n",
      "and in turn leads to safety and security in most of the applications. In this\n",
      "paper we make an attempt to characterize the solution space of a deep neural\n",
      "network in terms of three different subsets viz. weights belonging to exact\n",
      "trained patterns, weights belonging to generalized pattern set and weights\n",
      "belonging to adversarial pattern sets. We attempt to characterize the solution\n",
      "space with two seemingly different learning paradigms viz. the Deep Neural\n",
      "Networks and the Dense Associative Memory Model, which try to achieve learning\n",
      "via quite different mechanisms. We also show that adversarial attacks are\n",
      "generally less successful against Associative Memory Models than Deep Neural\n",
      "Networks.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5494\n",
      "Abstract:   In this work we propose a new deep learning tool called deep dictionary\n",
      "learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\n",
      "a time. This requires solving a simple (shallow) dictionary learning problem,\n",
      "the solution to this is well known. We apply the proposed technique on some\n",
      "benchmark deep learning datasets. We compare our results with other deep\n",
      "learning tools like stacked autoencoder and deep belief network; and state of\n",
      "the art supervised dictionary learning tools like discriminative KSVD and label\n",
      "consistent KSVD. Our method yields better results than all.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5485\n",
      "Abstract:   Deep neural networks have become invaluable tools for supervised machine\n",
      "learning, e.g., classification of text or images. While often offering superior\n",
      "results over traditional techniques and successfully expressing complicated\n",
      "patterns in data, deep architectures are known to be challenging to design and\n",
      "train such that they generalize well to new data. Important issues with deep\n",
      "architectures are numerical instabilities in derivative-based learning\n",
      "algorithms commonly called exploding or vanishing gradients. In this paper we\n",
      "propose new forward propagation techniques inspired by systems of Ordinary\n",
      "Differential Equations (ODE) that overcome this challenge and lead to\n",
      "well-posed learning problems for arbitrarily deep networks.\n",
      "  The backbone of our approach is our interpretation of deep learning as a\n",
      "parameter estimation problem of nonlinear dynamical systems. Given this\n",
      "formulation, we analyze stability and well-posedness of deep learning and use\n",
      "this new understanding to develop new network architectures. We relate the\n",
      "exploding and vanishing gradient phenomenon to the stability of the discrete\n",
      "ODE and present several strategies for stabilizing deep learning for very deep\n",
      "networks. While our new architectures restrict the solution space, several\n",
      "numerical experiments show their competitiveness with state-of-the-art\n",
      "networks.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5464\n",
      "Abstract:   Deep learning has become very popular for tasks such as predictive modeling\n",
      "and pattern recognition in handling big data. Deep learning is a powerful\n",
      "machine learning method that extracts lower level features and feeds them\n",
      "forward for the next layer to identify higher level features that improve\n",
      "performance. However, deep neural networks have drawbacks, which include many\n",
      "hyper-parameters and infinite architectures, opaqueness into results, and\n",
      "relatively slower convergence on smaller datasets. While traditional machine\n",
      "learning algorithms can address these drawbacks, they are not typically capable\n",
      "of the performance levels achieved by deep neural networks. To improve\n",
      "performance, ensemble methods are used to combine multiple base learners. Super\n",
      "learning is an ensemble that finds the optimal combination of diverse learning\n",
      "algorithms. This paper proposes deep super learning as an approach which\n",
      "achieves log loss and accuracy results competitive to deep neural networks\n",
      "while employing traditional machine learning algorithms in a hierarchical\n",
      "structure. The deep super learner is flexible, adaptable, and easy to train\n",
      "with good performance across different tasks using identical hyper-parameter\n",
      "values. Using traditional machine learning requires fewer hyper-parameters,\n",
      "allows transparency into results, and has relatively fast convergence on\n",
      "smaller datasets. Experimental results show that the deep super learner has\n",
      "superior performance compared to the individual base learners, single-layer\n",
      "ensembles, and in some cases deep neural networks. Performance of the deep\n",
      "super learner may further be improved with task-specific tuning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5463\n",
      "Abstract:   Currently, many applications in Machine Learning are based on define new\n",
      "models to extract more information about data, In this case Deep Reinforcement\n",
      "Learning with the most common application in video games like Atari, Mario, and\n",
      "others causes an impact in how to computers can learning by himself with only\n",
      "information called rewards obtained from any action. There is a lot of\n",
      "algorithms modeled and implemented based on Deep Recurrent Q-Learning proposed\n",
      "by DeepMind used in AlphaZero and Go. In this document, We proposed Deep\n",
      "Recurrent Double Q-Learning that is an implementation of Deep Reinforcement\n",
      "Learning using Double Q-Learning algorithms and Recurrent Networks like LSTM\n",
      "and DRQN.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5452\n",
      "Abstract:   How to understand deep learning systems remains an open problem. In this\n",
      "paper we propose that the answer may lie in the geometrization of deep\n",
      "networks. Geometrization is a bridge to connect physics, geometry, deep network\n",
      "and quantum computation and this may result in a new scheme to reveal the rule\n",
      "of the physical world. By comparing the geometry of image matching and deep\n",
      "networks, we show that geometrization of deep networks can be used to\n",
      "understand existing deep learning systems and it may also help to solve the\n",
      "interpretability problem of deep learning systems.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5451\n",
      "Abstract:   Deep learning is one of the new and important branches in machine learning.\n",
      "Deep learning refers to a set of algorithms that solve various problems such as\n",
      "images and texts by using various machine learning algorithms in multi-layer\n",
      "neural networks. Deep learning can be classified as a neural network from the\n",
      "general category, but there are many changes in the concrete realization. At\n",
      "the core of deep learning is feature learning, which is designed to obtain\n",
      "hierarchical information through hierarchical networks, so as to solve the\n",
      "important problems that previously required artificial design features. Deep\n",
      "Learning is a framework that contains several important algorithms. For\n",
      "different applications (images, voice, text), you need to use different network\n",
      "models to achieve better results. With the development of deep learning and the\n",
      "introduction of deep convolutional neural networks, the accuracy and speed of\n",
      "face recognition have made great strides. However, as we said above, the\n",
      "results from different networks and models are very different. In this paper,\n",
      "facial features are extracted by merging and comparing multiple models, and\n",
      "then a deep neural network is constructed to train and construct the combined\n",
      "features. In this way, the advantages of multiple models can be combined to\n",
      "mention the recognition accuracy. After getting a model with high accuracy, we\n",
      "build a product model. This article compares the pure-client model with the\n",
      "server-client model, analyzes the pros and cons of the two models, and analyzes\n",
      "the various commercial products that are required for the server-client model.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5449\n",
      "Abstract:   Deep learning has sparked a network of mutual interactions between different\n",
      "disciplines and AI. Naturally, each discipline focuses and interprets the\n",
      "workings of deep learning in different ways. This diversity of perspectives on\n",
      "deep learning, from neuroscience to statistical physics, is a rich source of\n",
      "inspiration that fuels novel developments in the theory and applications of\n",
      "machine learning. In this perspective, we collect and synthesize different\n",
      "intuitions scattered across several communities as for how deep learning works.\n",
      "In particular, we will briefly discuss the different perspectives that\n",
      "disciplines across mathematics, physics, computation, and neuroscience take on\n",
      "how deep learning does its tricks. Our discussion on each perspective is\n",
      "necessarily shallow due to the multiple views that had to be covered. The\n",
      "deepness in this case should come from putting all these faces of deep learning\n",
      "together in the reader's mind, so that one can look at the same problem from\n",
      "different angles.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5431\n",
      "Abstract:   Recently, deep learning techniques have enjoyed success in various multimedia\n",
      "applications, such as image classification and multi-modal data analysis. Large\n",
      "deep learning models are developed for learning rich representations of complex\n",
      "data. There are two challenges to overcome before deep learning can be widely\n",
      "adopted in multimedia and other applications. One is usability, namely the\n",
      "implementation of different models and training algorithms must be done by\n",
      "non-experts without much effort especially when the model is large and complex.\n",
      "The other is scalability, that is the deep learning system must be able to\n",
      "provision for a huge demand of computing resources for training large models\n",
      "with massive datasets. To address these two challenges, in this paper, we\n",
      "design a distributed deep learning platform called SINGA which has an intuitive\n",
      "programming model based on the common layer abstraction of deep learning\n",
      "models. Good scalability is achieved through flexible distributed training\n",
      "architecture and specific optimization techniques. SINGA runs on GPUs as well\n",
      "as on CPUs, and we show that it outperforms many other state-of-the-art deep\n",
      "learning systems. Our experience with developing and training deep learning\n",
      "models for real-life multimedia applications in SINGA shows that the platform\n",
      "is both usable and scalable.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5422\n",
      "Abstract:   Deep neural networks have introduced novel and useful tools to the machine\n",
      "learning community. Other types of classifiers can potentially make use of\n",
      "these tools as well to improve their performance and generality. This paper\n",
      "reviews the current state of the art for deep learning classifier technologies\n",
      "that are being used outside of deep neural networks. Non-network classifiers\n",
      "can employ many components found in deep neural network architectures. In this\n",
      "paper, we review the feature learning, optimization, and regularization methods\n",
      "that form a core of deep network technologies. We then survey non-neural\n",
      "network learning algorithms that make innovative use of these methods to\n",
      "improve classification. Because many opportunities and challenges still exist,\n",
      "we discuss directions that can be pursued to expand the area of deep learning\n",
      "for a variety of classification algorithms.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5406\n",
      "Abstract:   Deep learning has been applied to various tasks in the field of machine\n",
      "learning and has shown superiority to other common procedures such as kernel\n",
      "methods. To provide a better theoretical understanding of the reasons for its\n",
      "success, we discuss the performance of deep learning and other methods on a\n",
      "nonparametric regression problem with a Gaussian noise. Whereas existing\n",
      "theoretical studies of deep learning have been based mainly on mathematical\n",
      "theories of well-known function classes such as H\\\"{o}lder and Besov classes,\n",
      "we focus on function classes with discontinuity and sparsity, which are those\n",
      "naturally assumed in practice. To highlight the effectiveness of deep learning,\n",
      "we compare deep learning with a class of linear estimators representative of a\n",
      "class of shallow estimators. It is shown that the minimax risk of a linear\n",
      "estimator on the convex hull of a target function class does not differ from\n",
      "that of the original target function class. This results in the suboptimality\n",
      "of linear methods over a simple but non-convex function class, on which deep\n",
      "learning can attain nearly the minimax-optimal rate. In addition to this\n",
      "extreme case, we consider function classes with sparse wavelet coefficients. On\n",
      "these function classes, deep learning also attains the minimax rate up to log\n",
      "factors of the sample size, and linear methods are still suboptimal if the\n",
      "assumed sparsity is strong. We also point out that the parameter sharing of\n",
      "deep neural networks can remarkably reduce the complexity of the model in our\n",
      "setting.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5401\n",
      "Abstract:   In a physical neural system, where storage and processing are intimately\n",
      "intertwined, the rules for adjusting the synaptic weights can only depend on\n",
      "variables that are available locally, such as the activity of the pre- and\n",
      "post-synaptic neurons, resulting in local learning rules. A systematic\n",
      "framework for studying the space of local learning rules is obtained by first\n",
      "specifying the nature of the local variables, and then the functional form that\n",
      "ties them together into each learning rule. Such a framework enables also the\n",
      "systematic discovery of new learning rules and exploration of relationships\n",
      "between learning rules and group symmetries. We study polynomial local learning\n",
      "rules stratified by their degree and analyze their behavior and capabilities in\n",
      "both linear and non-linear units and networks. Stacking local learning rules in\n",
      "deep feedforward networks leads to deep local learning. While deep local\n",
      "learning can learn interesting representations, it cannot learn complex\n",
      "input-output functions, even when targets are available for the top layer.\n",
      "Learning complex input-output functions requires local deep learning where\n",
      "target information is communicated to the deep layers through a backward\n",
      "learning channel. The nature of the communicated information about the targets\n",
      "and the structure of the learning channel partition the space of learning\n",
      "algorithms. We estimate the learning channel capacity associated with several\n",
      "algorithms and show that backpropagation outperforms them by simultaneously\n",
      "maximizing the information rate and minimizing the computational cost, even in\n",
      "recurrent networks. The theory clarifies the concept of Hebbian learning,\n",
      "establishes the power and limitations of local learning rules, introduces the\n",
      "learning channel which enables a formal analysis of the optimality of\n",
      "backpropagation, and explains the sparsity of the space of learning rules\n",
      "discovered so far.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5388\n",
      "Abstract:   Deep learning tasks are often complicated and require a variety of components\n",
      "working together efficiently to perform well. Due to the often large scale of\n",
      "these tasks, there is a necessity to iterate quickly in order to attempt a\n",
      "variety of methods and to find and fix bugs. While participating in IARPA's\n",
      "Functional Map of the World challenge, we identified challenges along the\n",
      "entire deep learning pipeline and found various solutions to these challenges.\n",
      "In this paper, we present the performance, engineering, and deep learning\n",
      "considerations with processing and modeling data, as well as underlying\n",
      "infrastructure considerations that support large-scale deep learning tasks. We\n",
      "also discuss insights and observations with regard to satellite imagery and\n",
      "deep learning for image classification.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5386\n",
      "Abstract:   Deep learning (DL) algorithms are considered as a methodology of choice for\n",
      "remote-sensing image analysis over the past few years. Due to its effective\n",
      "applications, deep learning has also been introduced for automatic change\n",
      "detection and achieved great success. The present study attempts to provide a\n",
      "comprehensive review and a meta-analysis of the recent progress in this\n",
      "subfield. Specifically, we first introduce the fundamentals of deep learning\n",
      "methods which arefrequently adopted for change detection. Secondly, we present\n",
      "the details of the meta-analysis conducted to examine the status of change\n",
      "detection DL studies. Then, we focus on deep learning-based change detection\n",
      "methodologies for remote sensing images by giving a general overview of the\n",
      "existing methods. Specifically, these deep learning-based methods were\n",
      "classified into three groups; fully supervised learning-based methods, fully\n",
      "unsupervised learning-based methods and transfer learning-based techniques. As\n",
      "a result of these investigations, promising new directions were identified for\n",
      "future research. This study will contribute in several ways to our\n",
      "understanding of deep learning for change detection and will provide a basis\n",
      "for further research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5379\n",
      "Abstract:   Deep reinforcement learning is the combination of reinforcement learning (RL)\n",
      "and deep learning. This field of research has been able to solve a wide range\n",
      "of complex decision-making tasks that were previously out of reach for a\n",
      "machine. Thus, deep RL opens up many new applications in domains such as\n",
      "healthcare, robotics, smart grids, finance, and many more. This manuscript\n",
      "provides an introduction to deep reinforcement learning models, algorithms and\n",
      "techniques. Particular focus is on the aspects related to generalization and\n",
      "how deep RL can be used for practical applications. We assume the reader is\n",
      "familiar with basic machine learning concepts.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5368\n",
      "Abstract:   Advance in deep learning algorithms overshadows their security risk in\n",
      "software implementations. This paper discloses a set of vulnerabilities in\n",
      "popular deep learning frameworks including Caffe, TensorFlow, and Torch.\n",
      "Contrast to the small code size of deep learning models, these deep learning\n",
      "frameworks are complex and contain heavy dependencies on numerous open source\n",
      "packages. This paper considers the risks caused by these vulnerabilities by\n",
      "studying their impact on common deep learning applications such as voice\n",
      "recognition and image classifications. By exploiting these framework\n",
      "implementations, attackers can launch denial-of-service attacks that crash or\n",
      "hang a deep learning application, or control-flow hijacking attacks that cause\n",
      "either system compromise or recognition evasions. The goal of this paper is to\n",
      "draw attention on the software implementations and call for the community\n",
      "effort to improve the security of deep learning frameworks.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5362\n",
      "Abstract:   With the growth of deep learning, how to describe deep neural networks\n",
      "unifiedly is becoming an important issue. We first formalize neural networks\n",
      "mathematically with their directed graph representations, and prove a\n",
      "generation theorem about the induced networks of connected directed acyclic\n",
      "graphs. Then, we set up a unified framework for deep learning with capsule\n",
      "networks. This capsule framework could simplify the description of existing\n",
      "deep neural networks, and provide a theoretical basis of graphic designing and\n",
      "programming techniques for deep learning models, thus would be of great\n",
      "significance to the advancement of deep learning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5334\n",
      "Abstract:   In recent years, a specific machine learning method called deep learning has\n",
      "gained huge attraction, as it has obtained astonishing results in broad\n",
      "applications such as pattern recognition, speech recognition, computer vision,\n",
      "and natural language processing. Recent research has also been shown that deep\n",
      "learning techniques can be combined with reinforcement learning methods to\n",
      "learn useful representations for the problems with high dimensional raw data\n",
      "input. This chapter reviews the recent advances in deep reinforcement learning\n",
      "with a focus on the most used deep architectures such as autoencoders,\n",
      "convolutional neural networks and recurrent neural networks which have\n",
      "successfully been come together with the reinforcement learning framework.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5329\n",
      "Abstract:   Since 2012, Deep learning has revolutionized Artificial Intelligence and has\n",
      "achieved state-of-the-art outcomes in different domains, ranging from Image\n",
      "Classification to Speech Generation. Though it has many potentials, our current\n",
      "architectures come with the pre-requisite of large amounts of data. Few-Shot\n",
      "Learning (also known as one-shot learning) is a sub-field of machine learning\n",
      "that aims to create such models that can learn the desired objective with less\n",
      "data, similar to how humans learn. In this paper, we have reviewed some of the\n",
      "well-known deep learning-based approaches towards few-shot learning. We have\n",
      "discussed the recent achievements, challenges, and possibilities of improvement\n",
      "of few-shot learning based deep learning architectures. Our aim for this paper\n",
      "is threefold: (i) Give a brief introduction to deep learning architectures for\n",
      "few-shot learning with pointers to core references. (ii) Indicate how deep\n",
      "learning has been applied to the low-data regime, from data preparation to\n",
      "model training. and, (iii) Provide a starting point for people interested in\n",
      "experimenting and perhaps contributing to the field of few-shot learning by\n",
      "pointing out some useful resources and open-source code. Our code is available\n",
      "at Github: https://github.com/shruti-jadon/Hands-on-One-Shot-Learning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5325\n",
      "Abstract:   Deep learning is transforming many areas in science, and it has great\n",
      "potential in modeling molecular systems. However, unlike the mature deployment\n",
      "of deep learning in computer vision and natural language processing, its\n",
      "development in molecular modeling and simulations is still at an early stage,\n",
      "largely because the inductive biases of molecules are completely different from\n",
      "those of images or texts. Footed on these differences, we first reviewed the\n",
      "limitations of traditional deep learning models from the perspective of\n",
      "molecular physics, and wrapped up some relevant technical advancement at the\n",
      "interface between molecular modeling and deep learning. We do not focus merely\n",
      "on the ever more complex neural network models, instead, we emphasize the\n",
      "theories and ideas behind modern deep learning. We hope that transacting these\n",
      "ideas into molecular modeling will create new opportunities. For this purpose,\n",
      "we summarized several representative applications, ranging from supervised to\n",
      "unsupervised and reinforcement learning, and discussed their connections with\n",
      "the emerging trends in deep learning. Finally, we outlook promising directions\n",
      "which may help address the existing issues in the current framework of deep\n",
      "molecular modeling.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5322\n",
      "Abstract:   The lack of interpretability in current deep learning models causes serious\n",
      "concerns as they are extensively used for various life-critical applications.\n",
      "Hence, it is of paramount importance to develop interpretable deep learning\n",
      "models. In this paper, we consider the problem of blind deconvolution and\n",
      "propose a novel model-aware deep architecture that allows for the recovery of\n",
      "both the blur kernel and the sharp image from the blurred image. In particular,\n",
      "we propose the Deep Unfolded Richardson-Lucy (Deep-URL) framework -- an\n",
      "interpretable deep-learning architecture that can be seen as an amalgamation of\n",
      "classical estimation technique and deep neural network, and consequently leads\n",
      "to improved performance. Our numerical investigations demonstrate significant\n",
      "improvement compared to state-of-the-art algorithms.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5321\n",
      "Abstract:   Graphs provide a powerful means for representing complex interactions between\n",
      "entities. Recently, deep learning approaches are emerging for representing and\n",
      "modeling graph-structured data, although the conventional deep learning methods\n",
      "(such as convolutional neural networks and recurrent neural networks) have\n",
      "mainly focused on grid-structured inputs (image and audio). Leveraged by the\n",
      "capability of representation learning, deep learning based techniques are\n",
      "reporting promising results for graph applications by detecting structural\n",
      "characteristics of graphs in an automated fashion. In this paper, we attempt to\n",
      "advance deep learning for graph-structured data by incorporating another\n",
      "component, transfer learning. By transferring the intrinsic geometric\n",
      "information learned in the source domain, our approach can help us to construct\n",
      "a model for a new but related task in the target domain without collecting new\n",
      "data and without training a new model from scratch. We thoroughly test our\n",
      "approach with large-scale real corpora and confirm the effectiveness of the\n",
      "proposed transfer learning framework for deep learning on graphs. According to\n",
      "our experiments, transfer learning is most effective when the source and target\n",
      "domains bear a high level of structural similarity in their graph\n",
      "representations.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5314\n",
      "Abstract:   Deep learning networks have been trained to recognize speech, caption\n",
      "photographs and translate text between languages at high levels of performance.\n",
      "Although applications of deep learning networks to real world problems have\n",
      "become ubiquitous, our understanding of why they are so effective is lacking.\n",
      "These empirical results should not be possible according to sample complexity\n",
      "in statistics and non-convex optimization theory. However, paradoxes in the\n",
      "training and effectiveness of deep learning networks are being investigated and\n",
      "insights are being found in the geometry of high-dimensional spaces. A\n",
      "mathematical theory of deep learning would illuminate how they function, allow\n",
      "us to assess the strengths and weaknesses of different network architectures\n",
      "and lead to major improvements. Deep learning has provided natural ways for\n",
      "humans to communicate with digital devices and is foundational for building\n",
      "artificial general intelligence. Deep learning was inspired by the architecture\n",
      "of the cerebral cortex and insights into autonomy and general intelligence may\n",
      "be found in other brain regions that are essential for planning and survival,\n",
      "but major breakthroughs will be needed to achieve these goals.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5301\n",
      "Abstract:   Recently, the market on deep learning including not only software but also\n",
      "hardware is developing rapidly. Big data is collected through IoT devices and\n",
      "the industry world will analyze them to improve their manufacturing process.\n",
      "Deep Learning has the hierarchical network architecture to represent the\n",
      "complicated features of input patterns. Although deep learning can show the\n",
      "high capability of classification, prediction, and so on, the implementation on\n",
      "GPU devices are required. We may meet the trade-off between the higher\n",
      "precision by deep learning and the higher cost with GPU devices. We can success\n",
      "the knowledge extraction from the trained deep learning with high\n",
      "classification capability. The knowledge that can realize faster inference of\n",
      "pre-trained deep network is extracted as IF-THEN rules from the network signal\n",
      "flow given input data. Some experiment results with benchmark tests for time\n",
      "series data sets showed the effectiveness of our proposed method related to the\n",
      "computational speed.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5296\n",
      "Abstract:   This paper considers security risks buried in the data processing pipeline in\n",
      "common deep learning applications. Deep learning models usually assume a fixed\n",
      "scale for their training and input data. To allow deep learning applications to\n",
      "handle a wide range of input data, popular frameworks, such as Caffe,\n",
      "TensorFlow, and Torch, all provide data scaling functions to resize input to\n",
      "the dimensions used by deep learning models. Image scaling algorithms are\n",
      "intended to preserve the visual features of an image after scaling. However,\n",
      "common image scaling algorithms are not designed to handle human crafted\n",
      "images. Attackers can make the scaling outputs look dramatically different from\n",
      "the corresponding input images.\n",
      "  This paper presents a downscaling attack that targets the data scaling\n",
      "process in deep learning applications. By carefully crafting input data that\n",
      "mismatches with the dimension used by deep learning models, attackers can\n",
      "create deceiving effects. A deep learning application effectively consumes data\n",
      "that are not the same as those presented to users. The visual inconsistency\n",
      "enables practical evasion and data poisoning attacks to deep learning\n",
      "applications. This paper presents proof-of-concept attack samples to popular\n",
      "deep-learning-based image classification applications. To address the\n",
      "downscaling attacks, the paper also suggests multiple potential mitigation\n",
      "strategies.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5295\n",
      "Abstract:   The paper examines the potential of deep learning to support decisions in\n",
      "financial risk management. We develop a deep learning model for predicting\n",
      "whether individual spread traders secure profits from future trades. This task\n",
      "embodies typical modeling challenges faced in risk and behavior forecasting.\n",
      "Conventional machine learning requires data that is representative of the\n",
      "feature-target relationship and relies on the often costly development,\n",
      "maintenance, and revision of handcrafted features. Consequently, modeling\n",
      "highly variable, heterogeneous patterns such as trader behavior is challenging.\n",
      "Deep learning promises a remedy. Learning hierarchical distributed\n",
      "representations of the data in an automatic manner (e.g. risk taking behavior),\n",
      "it uncovers generative features that determine the target (e.g., trader's\n",
      "profitability), avoids manual feature engineering, and is more robust toward\n",
      "change (e.g. dynamic market conditions). The results of employing a deep\n",
      "network for operational risk forecasting confirm the feature learning\n",
      "capability of deep learning, provide guidance on designing a suitable network\n",
      "architecture and demonstrate the superiority of deep learning over machine\n",
      "learning and rule-based benchmarks.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5293\n",
      "Abstract:   Deep Learning is a very powerful machine learning model. Deep Learning trains\n",
      "a large number of parameters for multiple layers and is very slow when data is\n",
      "in large scale and the architecture size is large. Inspired from the shrinking\n",
      "technique used in accelerating computation of Support Vector Machines (SVM)\n",
      "algorithm and screening technique used in LASSO, we propose a shrinking Deep\n",
      "Learning with recall (sDLr) approach to speed up deep learning computation. We\n",
      "experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network\n",
      "(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data\n",
      "sets. Results show that the speedup using shrinking Deep Learning with recall\n",
      "(sDLr) can reach more than 2.0 while still giving competitive classification\n",
      "performance.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5292\n",
      "Abstract:   Deep learning has been widely applied and brought breakthroughs in speech\n",
      "recognition, computer vision, and many other domains. The involved deep neural\n",
      "network architectures and computational issues have been well studied in\n",
      "machine learning. But there lacks a theoretical foundation for understanding\n",
      "the approximation or generalization ability of deep learning methods generated\n",
      "by the network architectures such as deep convolutional neural networks having\n",
      "convolutional structures. Here we show that a deep convolutional neural network\n",
      "(CNN) is universal, meaning that it can be used to approximate any continuous\n",
      "function to an arbitrary accuracy when the depth of the neural network is large\n",
      "enough. This answers an open question in learning theory. Our quantitative\n",
      "estimate, given tightly in terms of the number of free parameters to be\n",
      "computed, verifies the efficiency of deep CNNs in dealing with large\n",
      "dimensional data. Our study also demonstrates the role of convolutions in deep\n",
      "CNNs.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5268\n",
      "Abstract:   Deep reinforcement learning is poised to revolutionise the field of AI and\n",
      "represents a step towards building autonomous systems with a higher level\n",
      "understanding of the visual world. Currently, deep learning is enabling\n",
      "reinforcement learning to scale to problems that were previously intractable,\n",
      "such as learning to play video games directly from pixels. Deep reinforcement\n",
      "learning algorithms are also applied to robotics, allowing control policies for\n",
      "robots to be learned directly from camera inputs in the real world. In this\n",
      "survey, we begin with an introduction to the general field of reinforcement\n",
      "learning, then progress to the main streams of value-based and policy-based\n",
      "methods. Our survey will cover central algorithms in deep reinforcement\n",
      "learning, including the deep $Q$-network, trust region policy optimisation, and\n",
      "asynchronous advantage actor-critic. In parallel, we highlight the unique\n",
      "advantages of deep neural networks, focusing on visual understanding via\n",
      "reinforcement learning. To conclude, we describe several current areas of\n",
      "research within the field.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5267\n",
      "Abstract:   Deep learning algorithms have been applied very successfully in recent years\n",
      "to a range of problems out of reach for classical solution paradigms.\n",
      "Nevertheless, there is no completely rigorous mathematical error and\n",
      "convergence analysis which explains the success of deep learning algorithms.\n",
      "The error of a deep learning algorithm can in many situations be decomposed\n",
      "into three parts, the approximation error, the generalization error, and the\n",
      "optimization error. In this work we estimate for a certain deep learning\n",
      "algorithm each of these three errors and combine these three error estimates to\n",
      "obtain an overall error analysis for the deep learning algorithm under\n",
      "consideration. In particular, we thereby establish convergence with a suitable\n",
      "convergence speed for the overall error of the deep learning algorithm under\n",
      "consideration. Our convergence speed analysis is far from optimal and the\n",
      "convergence speed that we establish is rather slow, increases exponentially in\n",
      "the dimensions, and, in particular, suffers from the curse of dimensionality.\n",
      "The main contribution of this work is, instead, to provide a full error\n",
      "analysis (i) which covers each of the three different sources of errors usually\n",
      "emerging in deep learning algorithms and (ii) which merges these three sources\n",
      "of errors into one overall error estimate for the considered deep learning\n",
      "algorithm.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5247\n",
      "Abstract:   Existing deep learning models may encounter great challenges in handling\n",
      "graph structured data. In this paper, we introduce a new deep learning model\n",
      "for graph data specifically, namely the deep loopy neural network.\n",
      "Significantly different from the previous deep models, inside the deep loopy\n",
      "neural network, there exist a large number of loops created by the extensive\n",
      "connections among nodes in the input graph data, which makes model learning an\n",
      "infeasible task. To resolve such a problem, in this paper, we will introduce a\n",
      "new learning algorithm for the deep loopy neural network specifically. Instead\n",
      "of learning the model variables based on the original model, in the proposed\n",
      "learning algorithm, errors will be back-propagated through the edges in a group\n",
      "of extracted spanning trees. Extensive numerical experiments have been done on\n",
      "several real-world graph datasets, and the experimental results demonstrate the\n",
      "effectiveness of both the proposed model and the learning algorithm in handling\n",
      "graph data.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5231\n",
      "Abstract:   Deep Learning has attracted significant attention in recent years. Here I\n",
      "present a brief overview of my first Deep Learner of 1991, and its historic\n",
      "context, with a timeline of Deep Learning highlights.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5229\n",
      "Abstract:   The application of deep learning in robotics leads to very specific problems\n",
      "and research questions that are typically not addressed by the computer vision\n",
      "and machine learning communities. In this paper we discuss a number of\n",
      "robotics-specific learning, reasoning, and embodiment challenges for deep\n",
      "learning. We explain the need for better evaluation metrics, highlight the\n",
      "importance and unique challenges for deep robotic learning in simulation, and\n",
      "explore the spectrum between purely data-driven and model-driven approaches. We\n",
      "hope this paper provides a motivating overview of important research directions\n",
      "to overcome the current limitations, and help fulfill the promising potentials\n",
      "of deep learning in robotics.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5225\n",
      "Abstract:   The past, present and future of deep learning is presented in this work.\n",
      "Given this landscape & roadmap, we predict that deep cortical learning will be\n",
      "the convergence of deep learning & cortical learning which builds an artificial\n",
      "cortical column ultimately.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5219\n",
      "Abstract:   The study of unsupervised learning can be generally divided into two\n",
      "categories: imitation learning and reinforcement learning. In imitation\n",
      "learning the machine learns by mimicking the behavior of an expert system\n",
      "whereas in reinforcement learning the machine learns via direct environment\n",
      "feedback. Traditional deep reinforcement learning takes a significant time\n",
      "before the machine starts to converge to an optimal policy. This paper proposes\n",
      "Augmented Q-Imitation-Learning, a method by which deep reinforcement learning\n",
      "convergence can be accelerated by applying Q-imitation-learning as the initial\n",
      "training process in traditional Deep Q-learning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5215\n",
      "Abstract:   This work deals with the use of emerging deep learning techniques in future\n",
      "wireless communication networks. It will be shown that data-driven approaches\n",
      "should not replace, but rather complement traditional design techniques based\n",
      "on mathematical models.\n",
      "  Extensive motivation is given for why deep learning based on artificial\n",
      "neural networks will be an indispensable tool for the design and operation of\n",
      "future wireless communications networks, and our vision of how artificial\n",
      "neural networks should be integrated into the architecture of future wireless\n",
      "communication networks is presented.\n",
      "  A thorough description of deep learning methodologies is provided, starting\n",
      "with the general machine learning paradigm, followed by a more in-depth\n",
      "discussion about deep learning and artificial neural networks, covering the\n",
      "most widely-used artificial neural network architectures and their training\n",
      "methods. Deep learning will also be connected to other major learning\n",
      "frameworks such as reinforcement learning and transfer learning.\n",
      "  A thorough survey of the literature on deep learning for wireless\n",
      "communication networks is provided, followed by a detailed description of\n",
      "several novel case-studies wherein the use of deep learning proves extremely\n",
      "useful for network design. For each case-study, it will be shown how the use of\n",
      "(even approximate) mathematical models can significantly reduce the amount of\n",
      "live data that needs to be acquired/measured to implement data-driven\n",
      "approaches. For each application, the merits of the proposed approaches will be\n",
      "demonstrated by a numerical analysis in which the implementation and training\n",
      "of the artificial neural network used to solve the problem is discussed.\n",
      "  Finally, concluding remarks describe those that in our opinion are the major\n",
      "directions for future research in this field.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5209\n",
      "Abstract:   Deep reinforcement learning (deep RL) has achieved superior performance in\n",
      "complex sequential tasks by using deep neural networks as function\n",
      "approximators to learn directly from raw input images. However, learning\n",
      "directly from raw images is data inefficient. The agent must learn feature\n",
      "representation of complex states in addition to learning a policy. As a result,\n",
      "deep RL typically suffers from slow learning speeds and often requires a\n",
      "prohibitively large amount of training time and data to reach reasonable\n",
      "performance, making it inapplicable to real-world settings where data is\n",
      "expensive. In this work, we improve data efficiency in deep RL by addressing\n",
      "one of the two learning goals, feature learning. We leverage supervised\n",
      "learning to pre-train on a small set of non-expert human demonstrations and\n",
      "empirically evaluate our approach using the asynchronous advantage actor-critic\n",
      "algorithms (A3C) in the Atari domain. Our results show significant improvements\n",
      "in learning speed, even when the provided demonstration is noisy and of low\n",
      "quality.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5197\n",
      "Abstract:   Classical linear metric learning methods have recently been extended along\n",
      "two distinct lines: deep metric learning methods for learning embeddings of the\n",
      "data using neural networks, and Bregman divergence learning approaches for\n",
      "extending learning Euclidean distances to more general divergence measures such\n",
      "as divergences over distributions. In this paper, we introduce deep Bregman\n",
      "divergences, which are based on learning and parameterizing functional Bregman\n",
      "divergences using neural networks, and which unify and extend these existing\n",
      "lines of work. We show in particular how deep metric learning formulations,\n",
      "kernel metric learning, Mahalanobis metric learning, and moment-matching\n",
      "functions for comparing distributions arise as special cases of these\n",
      "divergences in the symmetric setting. We then describe a deep learning\n",
      "framework for learning general functional Bregman divergences, and show in\n",
      "experiments that this method yields superior performance on benchmark datasets\n",
      "as compared to existing deep metric learning approaches. We also discuss novel\n",
      "applications, including a semi-supervised distributional clustering problem,\n",
      "and a new loss function for unsupervised data generation.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5193\n",
      "Abstract:   Most contemporary multi-task learning methods assume linear models. This\n",
      "setting is considered shallow in the era of deep learning. In this paper, we\n",
      "present a new deep multi-task representation learning framework that learns\n",
      "cross-task sharing structure at every layer in a deep network. Our approach is\n",
      "based on generalising the matrix factorisation techniques explicitly or\n",
      "implicitly used by many conventional MTL algorithms to tensor factorisation, to\n",
      "realise automatic learning of end-to-end knowledge sharing in deep networks.\n",
      "This is in contrast to existing deep learning approaches that need a\n",
      "user-defined multi-task sharing strategy. Our approach applies to both\n",
      "homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our\n",
      "deep multi-task representation learning in terms of both higher accuracy and\n",
      "fewer design choices.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5191\n",
      "Abstract:   Advancements in genomic research such as high-throughput sequencing\n",
      "techniques have driven modern genomic studies into \"big data\" disciplines. This\n",
      "data explosion is constantly challenging conventional methods used in genomics.\n",
      "In parallel with the urgent demand for robust algorithms, deep learning has\n",
      "succeeded in a variety of fields such as vision, speech, and text processing.\n",
      "Yet genomics entails unique challenges to deep learning since we are expecting\n",
      "from deep learning a superhuman intelligence that explores beyond our knowledge\n",
      "to interpret the genome. A powerful deep learning model should rely on\n",
      "insightful utilization of task-specific knowledge. In this paper, we briefly\n",
      "discuss the strengths of different deep learning models from a genomic\n",
      "perspective so as to fit each particular task with a proper deep architecture,\n",
      "and remark on practical considerations of developing modern deep learning\n",
      "architectures for genomics. We also provide a concise review of deep learning\n",
      "applications in various aspects of genomic research, as well as pointing out\n",
      "potential opportunities and obstacles for future genomics applications.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5182\n",
      "Abstract:   Understanding the effect of depth in deep learning is a critical problem. In\n",
      "this work, we utilize the Fourier analysis to empirically provide a promising\n",
      "mechanism to understand why feedforward deeper learning is faster. To this end,\n",
      "we separate a deep neural network, trained by normal stochastic gradient\n",
      "descent, into two parts during analysis, i.e., a pre-condition component and a\n",
      "learning component, in which the output of the pre-condition one is the input\n",
      "of the learning one. We use a filtering method to characterize the frequency\n",
      "distribution of a high-dimensional function. Based on experiments of deep\n",
      "networks and real dataset, we propose a deep frequency principle, that is, the\n",
      "effective target function for a deeper hidden layer biases towards lower\n",
      "frequency during the training. Therefore, the learning component effectively\n",
      "learns a lower frequency function if the pre-condition component has more\n",
      "layers. Due to the well-studied frequency principle, i.e., deep neural networks\n",
      "learn lower frequency functions faster, the deep frequency principle provides a\n",
      "reasonable explanation to why deeper learning is faster. We believe these\n",
      "empirical studies would be valuable for future theoretical studies of the\n",
      "effect of depth in deep learning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5181\n",
      "Abstract:   Over the past few years, we have seen fundamental breakthroughs in core\n",
      "problems in machine learning, largely driven by advances in deep neural\n",
      "networks. At the same time, the amount of data collected in a wide array of\n",
      "scientific domains is dramatically increasing in both size and complexity.\n",
      "Taken together, this suggests many exciting opportunities for deep learning\n",
      "applications in scientific settings. But a significant challenge to this is\n",
      "simply knowing where to start. The sheer breadth and diversity of different\n",
      "deep learning techniques makes it difficult to determine what scientific\n",
      "problems might be most amenable to these methods, or which specific combination\n",
      "of methods might offer the most promising first approach. In this survey, we\n",
      "focus on addressing this central issue, providing an overview of many widely\n",
      "used deep learning models, spanning visual, sequential and graph structured\n",
      "data, associated tasks and different training methods, along with techniques to\n",
      "use deep learning with less data and better interpret these complex models ---\n",
      "two central considerations for many scientific use cases. We also include\n",
      "overviews of the full design process, implementation tips, and links to a\n",
      "plethora of tutorials, research summaries and open-sourced deep learning\n",
      "pipelines and pretrained models, developed by the community. We hope that this\n",
      "survey will help accelerate the use of deep learning across different\n",
      "scientific domains.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5179\n",
      "Abstract:   Deep learning methods are often difficult to apply in the legal domain due to\n",
      "the large amount of labeled data required by deep learning methods. A recent\n",
      "new trend in the deep learning community is the application of multi-task\n",
      "models that enable single deep neural networks to perform more than one task at\n",
      "the same time, for example classification and translation tasks. These powerful\n",
      "novel models are capable of transferring knowledge among different tasks or\n",
      "training sets and therefore could open up the legal domain for many deep\n",
      "learning applications. In this paper, we investigate the transfer learning\n",
      "capabilities of such a multi-task model on a classification task on the\n",
      "publicly available Kaggle toxic comment dataset for classifying illegal\n",
      "comments and we can report promising results.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5163\n",
      "Abstract:   Learning-based approaches for solving large sequential decision making\n",
      "problems have become popular in recent years. The resulting agents perform\n",
      "differently and their characteristics depend on those of the underlying\n",
      "learning approach. Here, we consider a benchmark planning problem from the\n",
      "reinforcement learning domain, the Racetrack, to investigate the properties of\n",
      "agents derived from different deep (reinforcement) learning approaches. We\n",
      "compare the performance of deep supervised learning, in particular imitation\n",
      "learning, to reinforcement learning for the Racetrack model. We find that\n",
      "imitation learning yields agents that follow more risky paths. In contrast, the\n",
      "decisions of deep reinforcement learning are more foresighted, i.e., avoid\n",
      "states in which fatal decisions are more likely. Our evaluations show that for\n",
      "this sequential decision making problem, deep reinforcement learning performs\n",
      "best in many aspects even though for imitation learning optimal decisions are\n",
      "considered.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5150\n",
      "Abstract:   Machine learning has made tremendous progress in recent years and received\n",
      "large amounts of public attention. Though we are still far from designing a\n",
      "full artificially intelligent agent, machine learning has brought us many\n",
      "applications in which computers solve human learning tasks remarkably well.\n",
      "Much of this progress comes from a recent trend within machine learning, called\n",
      "deep learning. Deep learning models are responsible for many state-of-the-art\n",
      "applications of machine learning. Despite their success, deep learning models\n",
      "are hard to train, very difficult to understand, and often times so complex\n",
      "that training is only possible on very large GPU clusters. Lots of work has\n",
      "been done on enabling neural networks to learn efficiently. However, the design\n",
      "and architecture of such neural networks is often done manually through trial\n",
      "and error and expert knowledge. This thesis inspects different approaches,\n",
      "existing and novel, to automate the design of deep feedforward neural networks\n",
      "in an attempt to create less complex models with good performance that take\n",
      "away the burden of deciding on an architecture and make it more efficient to\n",
      "design and train such deep networks.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5143\n",
      "Abstract:   Deep learning models are considered to be state-of-the-art in many offline\n",
      "machine learning tasks. However, many of the techniques developed are not\n",
      "suitable for online learning tasks. The problem of using deep learning models\n",
      "with sequential data becomes even harder when several loss functions need to be\n",
      "considered simultaneously, as in many real-world applications. In this paper,\n",
      "we, therefore, propose a novel online deep learning training procedure which\n",
      "can be used regardless of the neural network's architecture, aiming to deal\n",
      "with the multiple objectives case. We demonstrate and show the effectiveness of\n",
      "our algorithm on the Neyman-Pearson classification problem on several benchmark\n",
      "datasets.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5108\n",
      "Abstract:   In this paper we develop a statistical theory and an implementation of deep\n",
      "learning models. We show that an elegant variable splitting scheme for the\n",
      "alternating direction method of multipliers optimises a deep learning\n",
      "objective. We allow for non-smooth non-convex regularisation penalties to\n",
      "induce sparsity in parameter weights. We provide a link between traditional\n",
      "shallow layer statistical models such as principal component and sliced inverse\n",
      "regression and deep layer models. We also define the degrees of freedom of a\n",
      "deep learning predictor and a predictive MSE criteria to perform model\n",
      "selection for comparing architecture designs. We focus on deep multiclass\n",
      "logistic learning although our methods apply more generally. Our results\n",
      "suggest an interesting and previously under-exploited relationship between deep\n",
      "learning and proximal splitting techniques. To illustrate our methodology, we\n",
      "provide a multi-class logit classification analysis of Fisher's Iris data where\n",
      "we illustrate the convergence of our algorithm. Finally, we conclude with\n",
      "directions for future research.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5103\n",
      "Abstract:   Deep learning has been one of the most prominent machine learning techniques\n",
      "nowadays, being the state-of-the-art on a broad range of applications where\n",
      "automatic feature extraction is needed. Many such applications also demand\n",
      "varying costs for different types of mis-classification errors, but it is not\n",
      "clear whether or how such cost information can be incorporated into deep\n",
      "learning to improve performance. In this work, we propose a novel cost-aware\n",
      "algorithm that takes into account the cost information into not only the\n",
      "training stage but also the pre-training stage of deep learning. The approach\n",
      "allows deep learning to conduct automatic feature extraction with the cost\n",
      "information effectively. Extensive experimental results demonstrate that the\n",
      "proposed approach outperforms other deep learning models that do not digest the\n",
      "cost information in the pre-training stage.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5097\n",
      "Abstract:   This paper introduces Deep Incremental Boosting, a new technique derived from\n",
      "AdaBoost, specifically adapted to work with Deep Learning methods, that reduces\n",
      "the required training time and improves generalisation. We draw inspiration\n",
      "from Transfer of Learning approaches to reduce the start-up time to training\n",
      "each incremental Ensemble member. We show a set of experiments that outlines\n",
      "some preliminary results on some common Deep Learning datasets and discuss the\n",
      "potential improvements Deep Incremental Boosting brings to traditional Ensemble\n",
      "methods in Deep Learning.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5029\n",
      "Abstract:   The practical impact of deep learning on complex supervised learning problems\n",
      "has been significant, so much so that almost every Artificial Intelligence\n",
      "problem, or at least a portion thereof, has been somehow recast as a deep\n",
      "learning problem. The applications appeal is significant, but this appeal is\n",
      "increasingly challenged by what some call the challenge of explainability, or\n",
      "more generally the more traditional challenge of debuggability: if the outcomes\n",
      "of a deep learning process produce unexpected results (e.g., less than expected\n",
      "performance of a classifier), then there is little available in the way of\n",
      "theories or tools to help investigate the potential causes of such unexpected\n",
      "behavior, especially when this behavior could impact people's lives. We\n",
      "describe a preliminary framework to help address this issue, which we call\n",
      "\"deep visual explanation\" (DVE). \"Deep,\" because it is the development and\n",
      "performance of deep neural network models that we want to understand. \"Visual,\"\n",
      "because we believe that the most rapid insight into a complex multi-dimensional\n",
      "model is provided by appropriate visualization techniques, and \"Explanation,\"\n",
      "because in the spectrum from instrumentation by inserting print statements to\n",
      "the abductive inference of explanatory hypotheses, we believe that the key to\n",
      "understanding deep learning relies on the identification and exposure of\n",
      "hypotheses about the performance behavior of a learned deep model. In the\n",
      "exposition of our preliminary framework, we use relatively straightforward\n",
      "image classification examples and a variety of choices on initial configuration\n",
      "of a deep model building scenario. By careful but not complicated\n",
      "instrumentation, we expose classification outcomes of deep models using\n",
      "visualization, and also show initial results for one potential application of\n",
      "interpretability.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5029\n",
      "Abstract:   We develop a two-stage deep learning pipeline architecture to estimate the\n",
      "uplink massive MIMO channel with one-bit ADCs. This deep learning pipeline is\n",
      "composed of two separate generative deep learning models. The first one is a\n",
      "supervised learning model and designed to compensate for the quantization loss.\n",
      "The second one is an unsupervised learning model and optimized for denoising.\n",
      "Our results show that the proposed deep learning-based channel estimator can\n",
      "significantly outperform other state-of-the-art channel estimators for one-bit\n",
      "quantized massive MIMO systems. In particular, our design provides 5-10 dB gain\n",
      "in channel estimation error. Furthermore, it requires a reasonable amount of\n",
      "pilots, on the order of 20 per coherence time interval.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5027\n",
      "Abstract:   Due to the capability of deep learning to perform well in high dimensional\n",
      "problems, deep reinforcement learning agents perform well in challenging tasks\n",
      "such as Atari 2600 games. However, clearly explaining why a certain action is\n",
      "taken by the agent can be as important as the decision itself. Deep\n",
      "reinforcement learning models, as other deep learning models, tend to be opaque\n",
      "in their decision-making process. In this work, we propose to make deep\n",
      "reinforcement learning more transparent by visualizing the evidence on which\n",
      "the agent bases its decision. In this work, we emphasize the importance of\n",
      "producing a justification for an observed action, which could be applied to a\n",
      "black-box decision agent.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5020\n",
      "Abstract:   In recent studies in hyperspectral imaging, biometrics and energy analytics,\n",
      "the framework of deep dictionary learning has shown promise. Deep dictionary\n",
      "learning outperforms other traditional deep learning tools when training data\n",
      "is limited; therefore hyperspectral imaging is one such example that benefits\n",
      "from this framework. Most of the prior studies were based on the unsupervised\n",
      "formulation; and in all cases, the training algorithm was greedy and hence\n",
      "sub-optimal. This is the first work that shows how to learn the deep dictionary\n",
      "learning problem in a joint fashion. Moreover, we propose a new discriminative\n",
      "penalty to the said framework. The third contribution of this work is showing\n",
      "how to incorporate stochastic regularization techniques into the deep\n",
      "dictionary learning framework. Experimental results on hyperspectral image\n",
      "classification shows that the proposed technique excels over all\n",
      "state-of-the-art deep and shallow (traditional) learning based methods\n",
      "published in recent times.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5019\n",
      "Abstract:   By bridging deep networks and physics, the programme of geometrization of\n",
      "deep networks was proposed as a framework for the interpretability of deep\n",
      "learning systems. Following this programme we can apply two key ideas of\n",
      "physics, the geometrization of physics and the least action principle, on deep\n",
      "networks and deliver a new picture of deep networks: deep networks as memory\n",
      "space of information, where the capacity, robustness and efficiency of the\n",
      "memory are closely related with the complexity, generalization and\n",
      "disentanglement of deep networks. The key components of this understanding\n",
      "include:(1) a Fisher metric based formulation of the network complexity; (2)the\n",
      "least action (complexity=action) principle on deep networks and (3)the geometry\n",
      "built on deep network configurations. We will show how this picture will bring\n",
      "us a new understanding of the interpretability of deep learning systems.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5013\n",
      "Abstract:   Despite the recent success of deep transfer learning approaches in NLP, there\n",
      "is a lack of quantitative studies demonstrating the gains these models offer in\n",
      "low-shot text classification tasks over existing paradigms. Deep transfer\n",
      "learning approaches such as BERT and ULMFiT demonstrate that they can beat\n",
      "state-of-the-art results on larger datasets, however when one has only 100-1000\n",
      "labelled examples per class, the choice of approach is less clear, with\n",
      "classical machine learning and deep transfer learning representing valid\n",
      "options. This paper compares the current best transfer learning approach with\n",
      "top classical machine learning approaches on a trinary sentiment classification\n",
      "task to assess the best paradigm. We find that BERT, representing the best of\n",
      "deep transfer learning, is the best performing approach, outperforming top\n",
      "classical machine learning algorithms by 9.7% on average when trained with 100\n",
      "examples per class, narrowing to 1.8% at 1000 labels per class. We also show\n",
      "the robustness of deep transfer learning in moving across domains, where the\n",
      "maximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross\n",
      "domain, compared to classical machine learning which loses up to 20.6%.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Similarity Score: 0.5011\n",
      "Abstract:   Deep learning is the mainstream technique for many machine learning tasks,\n",
      "including image recognition, machine translation, speech recognition, and so\n",
      "on. It has outperformed conventional methods in various fields and achieved\n",
      "great successes. Unfortunately, the understanding on how it works remains\n",
      "unclear. It has the central importance to lay down the theoretic foundation for\n",
      "deep learning.\n",
      "  In this work, we give a geometric view to understand deep learning: we show\n",
      "that the fundamental principle attributing to the success is the manifold\n",
      "structure in data, namely natural high dimensional data concentrates close to a\n",
      "low-dimensional manifold, deep learning learns the manifold and the probability\n",
      "distribution on it.\n",
      "  We further introduce the concepts of rectified linear complexity for deep\n",
      "neural network measuring its learning capability, rectified linear complexity\n",
      "of an embedding manifold describing the difficulty to be learned. Then we show\n",
      "for any deep neural network with fixed architecture, there exists a manifold\n",
      "that cannot be learned by the network. Finally, we propose to apply optimal\n",
      "mass transportation theory to control the probability distribution in the\n",
      "latent space.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input query\n",
    "query = \"Deep learning is\"\n",
    "query_vec = vectorizer.transform([query])\n",
    "\n",
    "# Compute similarities\n",
    "similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "\n",
    "# Sort the abstracts by similarity in descending order\n",
    "sorted_indices = similarities.argsort()[::-1]\n",
    "\n",
    "# Display abstracts with similarity score > 0.5\n",
    "print(\"Abstracts with similarity score > 0.5:\\n\")\n",
    "for idx in sorted_indices:\n",
    "    if similarities[idx] > 0.5:  # Check if similarity score is greater than 0.5\n",
    "        print(f\"Similarity Score: {similarities[idx]:.4f}\")\n",
    "        print(f\"Abstract: {df['abstract'].iloc[idx]}\\n\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3bf456-c0b2-4d56-af0a-028f446f980a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/naufalputra/Downloads/chatbot_pka/notebook/../model/tfidf_matrix.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, os.path.join(os.getcwd(), '..', 'model', 'tfidf_vectorizer.pkl'))\n",
    "\n",
    "# Save the TF-IDF matrix\n",
    "joblib.dump(tfidf_matrix, os.path.join(os.getcwd(), '..', 'model', 'tfidf_matrix.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c43268-eff6-4c92-8a60-0cf822122e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
